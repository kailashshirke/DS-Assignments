{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kailashshirke/DS-Assignments/blob/main/Assignment_16_Neural_Network_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "YFmNPjAkhVrv"
      },
      "source": [
        "PREDICT THE BURNED AREA OF FOREST FIRES WITH NEURAL NETWORKS\n",
        "\n",
        "month\tmonth of the year: 'jan' to 'dec'\n",
        "day\tday of the week: 'mon' to 'sun'\n",
        "FFMC\tFFMC index from the FWI system: 18.7 to 96.20\n",
        "DMC\tDMC index from the FWI system: 1.1 to 291.3\n",
        "DC\tDC index from the FWI system: 7.9 to 860.6\n",
        "ISI\tISI index from the FWI system: 0.0 to 56.10\n",
        "temp\ttemperature in Celsius degrees: 2.2 to 33.30\n",
        "RH\trelative humidity in %: 15.0 to 100\n",
        "wind\twind speed in km/h: 0.40 to 9.40\n",
        "rain\toutside rain in mm/m2 : 0.0 to 6.4\n",
        "Size_Categorie \tthe burned area of the forest ( Small , Large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC2MbOkchVrw"
      },
      "source": [
        "#### Dataset: forestfires.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isbjswAkhVrw"
      },
      "outputs": [],
      "source": [
        "# Importig Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTs3SHgahVrx"
      },
      "outputs": [],
      "source": [
        "# Loading dataset\n",
        "\n",
        "data = pd.read_csv('forestfires.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM-RjIK2hVry"
      },
      "source": [
        "### EDA & Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7eEVrj-hVry"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt0uHiSAhVr0"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS0q3qXzhVr0"
      },
      "outputs": [],
      "source": [
        "data.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYtkAw-uhVr1"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rABe1ujhVr2"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMyWQMjEhVr2"
      },
      "outputs": [],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4FV8XbAhVr2"
      },
      "outputs": [],
      "source": [
        "# Dropping columns which are not required\n",
        "\n",
        "data = data.drop(['dayfri', 'daymon', 'daysat', 'daysun', 'daythu','daytue', 'daywed', 'monthapr', 'monthaug', 'monthdec', \n",
        "                  'monthfeb','monthjan', 'monthjul', 'monthjun', 'monthmar', 'monthmay', 'monthnov','monthoct','monthsep'], \n",
        "                 axis = 1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GCg4Yo2LhVr2"
      },
      "outputs": [],
      "source": [
        "# Checking how much datapoints are having small and large area\n",
        "data.size_category.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTLu-HhVr3"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x = 'size_category', data = data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "E5t5DJFVhVr4"
      },
      "outputs": [],
      "source": [
        "# Checking for which value of area is categorised into large and small by creating crosstab between area and size_category\n",
        "pd.crosstab(data.area, data.size_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PliGYZoZhVr4"
      },
      "outputs": [],
      "source": [
        "# Plotting Month Vs. temp plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "sns.set(style = \"darkgrid\", font_scale = 1.3)\n",
        "month_temp = sns.barplot(x = 'month', y = 'temp', data = data,\n",
        "                         order = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'], palette = 'winter');\n",
        "month_temp.set(title = \"Month Vs Temp Barplot\", xlabel = \"Months\", ylabel = \"Temperature\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8KepxYphVr4"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "sns.set(style = 'whitegrid', font_scale = 1.3)\n",
        "day = sns.countplot(data['day'], order = ['sun' ,'mon', 'tue', 'wed', 'thu', 'fri', 'sat'], palette = 'spring')\n",
        "day.set(title = 'Countplot for the weekdays', xlabel = 'Days', ylabel = 'Count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSWHLs8yhVr5"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(data.corr(), annot=True, cmap=\"inferno\")\n",
        "ax = plt.gca()\n",
        "ax.set_title(\"HeatMap of Features for the Classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0B8avZ6hVr5"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "w9EROhAghVr5"
      },
      "outputs": [],
      "source": [
        "# Encoding month and day features\n",
        "\n",
        "data.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),\n",
        "                           (1,2,3,4,5,6,7,8,9,10,11,12), inplace=True)\n",
        "data.day.replace(('mon','tue','wed','thu','fri','sat','sun'),(1,2,3,4,5,6,7), inplace=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Yk4gfDv3hVr5"
      },
      "outputs": [],
      "source": [
        "# Encoding target variable 'size category'\n",
        "\n",
        "data.size_category.replace(('small', 'large'), (0, 1), inplace = True)\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SWqn6ZOhVr5"
      },
      "outputs": [],
      "source": [
        "data.corr()['size_category'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnEdm4PUhVr6"
      },
      "outputs": [],
      "source": [
        "# Standardizing data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ubQzr-6hVr6"
      },
      "outputs": [],
      "source": [
        "scaler.fit(data.drop('size_category',axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr17RB4ehVr6"
      },
      "outputs": [],
      "source": [
        "scaled_features=scaler.transform(data.drop('size_category',axis=1))\n",
        "data_head=pd.DataFrame(scaled_features,columns=data.columns[:-1])\n",
        "data_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLxG4FlhhVr6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Splitting data into test data and train data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_head,data['size_category'], test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1l7QzMjvhVr7"
      },
      "outputs": [],
      "source": [
        "print('Shape of x_train: ', x_train.shape)\n",
        "print('Shape of x_test: ', x_test.shape)\n",
        "print('Shape of y_train: ', y_train.shape)\n",
        "print('Shape of y_test: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCI2UtoKhVr7"
      },
      "source": [
        "### Artificial Neural Network Model - Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y44oTb6ihVr8"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PDCoN2yhVr8"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=11, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(10, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TcnP5YLhVr9"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yKHyve-whVr9"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "model.fit(x_train,y_train, epochs=100, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od9fcJAhhVr-"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFQuaH9yhVr_"
      },
      "source": [
        "### Artificial Neural Network Model - Tuning of Hyperparameters batch size and epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMOlZStfhVsA"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary packages\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRucgGsPhVsA"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=11, kernel_initializer='uniform', activation='relu'))\n",
        "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "    \n",
        "    adam=adam_v2.Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g4tSulUPhVsA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create the model\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
        "# Define the grid search parameters\n",
        "batch_size = [10,20,40]\n",
        "epochs = [10,50,100]\n",
        "# Make a dictionary of the grid search parameters\n",
        "param_grid = dict(batch_size = batch_size,epochs = epochs)\n",
        "# Build and fit the GridSearchCV\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NoROPJ_qhVsA"
      },
      "outputs": [],
      "source": [
        "# Summarize the results\n",
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGN4f6lwhVsB"
      },
      "source": [
        "## Artificial Neural Network Model - Tuning of All Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYcCjcqzhVsB",
        "outputId": "a32585a5-151f-4c44-e98a-cebc297419af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 446/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=1.000 total time=   6.0s\n",
            "[CV 2/5; 446/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
            "[CV 2/5; 446/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.958 total time=   6.0s\n",
            "[CV 3/5; 446/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
            "[CV 3/5; 446/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.944 total time=   6.0s\n",
            "[CV 4/5; 446/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
            "[CV 4/5; 446/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=1.000 total time=   4.7s\n",
            "[CV 5/5; 446/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
            "[CV 5/5; 446/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.917 total time=   6.0s\n",
            "[CV 1/5; 447/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
            "[CV 1/5; 447/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.973 total time=   4.9s\n",
            "[CV 2/5; 447/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
            "[CV 2/5; 447/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.944 total time=   4.7s\n",
            "[CV 3/5; 447/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
            "[CV 3/5; 447/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.972 total time=   6.0s\n",
            "[CV 4/5; 447/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
            "[CV 4/5; 447/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.986 total time=   5.0s\n",
            "[CV 5/5; 447/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=100, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        }
      ],
      "source": [
        "def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neuron1,input_dim = 11,kernel_initializer = init,activation = activation_function))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "    adam=adam_v2.Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "\n",
        "batch_size = [10,20,40]\n",
        "epochs = [10,50,100]\n",
        "learning_rate = [0.001,0.01,0.1]\n",
        "dropout_rate = [0.0,0.1,0.2]\n",
        "activation_function = ['softmax','relu','tanh','linear']\n",
        "init = ['uniform','normal','zero']\n",
        "neuron1 = [4,8,16]\n",
        "neuron2 = [2,4,8]\n",
        "\n",
        "# Make a dictionary of the grid search parameters\n",
        "\n",
        "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
        "                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)\n",
        "\n",
        "# Build and fit the GridSearchCV\n",
        "\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(x_train, y_train)\n",
        "\n",
        "# Summarize the results\n",
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMkTsLvuhVsB"
      },
      "source": [
        "#### Best : 0.9916666626930237, using {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhwtNisghVsB"
      },
      "source": [
        "### Building final model with above best parameters obtained from tuning hyper paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjK0V2UjhVsB"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(4,input_dim = 11,kernel_initializer = 'normal',activation = 'tanh'))\n",
        "final_model.add(Dropout(0.1))\n",
        "final_model.add(Dense(2,input_dim = 4,kernel_initializer = 'normal',activation = 'tanh'))\n",
        "final_model.add(Dropout(0.1))\n",
        "final_model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "adam=adam_v2.Adam(learning_rate = 0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCXKVY01hVsC"
      },
      "outputs": [],
      "source": [
        "# Compile Model\n",
        "final_model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UR1WQeuLhVsC"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "final_model.fit(x_train,y_train, epochs=50, batch_size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtj7JGlrhVsC"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "scores = final_model.evaluate(x_test, y_test)\n",
        "print(\"%s: %.2f%%\" % (final_model.metrics_names[1], scores[1]*100))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Assignment 16. Neural Network - 01.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}